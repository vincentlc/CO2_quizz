{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d6b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d14ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76dfbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "headers = {\"Authorization\": \"Bearer \" + secrets.token}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def easy_query(instruction):\n",
    "    text = instruction\n",
    "    output = query({\"inputs\": text,})\n",
    "    assert len(output) > 0\n",
    "    generated_text = output[0].get(\"generated_text\", text)\n",
    "    return generated_text.split(\"[/INST]\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad99c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a Large Language Model trained by Mistral AI, a leading AI company based in Paris. I am designed to generate human-like text based on the prompts I am given. I can answer questions, write essays, summarize texts, translate languages, and even generate creative ideas. I am constantly learning and updating my knowledge to provide the most accurate and relevant responses possible. I am intended to be a helpful and respectful tool to assist and empower users in their daily tasks and decisions.\n"
     ]
    }
   ],
   "source": [
    "text = \"[INST]Tell me about you[/INST]\"\n",
    "response = easy_query(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cef9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between an apple and an orange, which has a higher footprint?\n"
     ]
    }
   ],
   "source": [
    "initial_text = \"\"\"[INST]Your task is to write a question about the carbon footprint of objects.\n",
    "You will ask to compare two objects.\n",
    "You will respond with one question.\n",
    "The question will be less than 10 words long.\n",
    "You will not add anything else.\n",
    "###\n",
    "Here are the objects and their carbon footprint:\n",
    "Banana: 3\n",
    "Apple: 2\n",
    "Orange: 9\n",
    "Mango: 11\n",
    "###\n",
    "Here are examples:\n",
    "Which has a higher carbon footprint between a car and a bus?\n",
    "Which has the lowest carbon footprint, a banana or an apple?\n",
    "[/INST]\"\"\"\n",
    "\n",
    "response = easy_query(initial_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872201b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_answer = \"I think it's the orange\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937d989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you are correct. The carbon footprint of an orange is 9, while an apple is 2. Therefore, an orange has a higher carbon footprint than an apple.\n"
     ]
    }
   ],
   "source": [
    "text = \"<s>\" + initial_text + response + \"</s>\" + \"[INST]\" + user_answer + \"[/INST]\"\n",
    "\n",
    "response = easy_query(text)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
