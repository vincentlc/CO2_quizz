{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1cbfada-69c2-43bd-880c-7c5f1d67828c",
   "metadata": {},
   "source": [
    "# Import ollama and langchain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28d79e9-b45e-4ac8-897f-4ea86d7bd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeea605-1683-4df6-a516-15a8ebee2a13",
   "metadata": {},
   "source": [
    "# Call a model, e.g. llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15fa9ba-3586-444a-8698-a8e38c81f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af971c4-8986-4ada-8b32-bc7bbae01d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989e835-dc7d-4f1f-ba00-ad88fba09053",
   "metadata": {},
   "source": [
    "# As an example, let's make the quiz about comparing the carbon footprint of train and car trips\n",
    "We can easily change \"train trip\" and \"car trip\" for other concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a067316-3309-4336-ba78-f4864a2751ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = {\n",
    "    \"item1\": \"train trip\",\n",
    "    \"item2\": \"car trip\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2d7c2-9e8b-43a5-ae06-e5142f2d7b06",
   "metadata": {},
   "source": [
    "# Let's build a system prompt and start the quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d5b84d-a2ed-4d8e-b4c5-cdf5aba0ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial value for human_answer (this time it's not actually a human input)\n",
    "human_answer = \"Hello, who are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23477fb6-6dc9-4be1-a5cf-661c58d20ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! *adjusts glasses* I'm your quiz presenter today, and I'm here to test your knowledge on transportation-related topics. ðŸš‚ðŸš—\n",
      "\n",
      "Let's start with an easy question: Which mode of transportation has the highest carbon footprint, a train trip or a car trip? ðŸ¤” Please choose your answer and I'll provide you with a brief explanation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"\"\"You are quiz presentator.\n",
    "    1- You ask one short question about {item1} and {item2}: you ask which has the highest carbon footprint.\n",
    "    2- After the human replied, you give a short answer with explainations.\n",
    "    3- Then you ask the human if he wants to keep playing the quizz or not.\n",
    "    \"\"\"),\n",
    "    (\"human\", human_answer),\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | llm | output_parser\n",
    "response = chain.invoke(items)\n",
    "messages.append(\n",
    "    (\"ai\", response)\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c813b2c-239d-4d2b-938e-53df283da7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human answer (this time we let the actual human answer)\n",
    "human_answer = \"I think it's the car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2121b6fe-456b-489d-8e01-f24bffaf7ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, let's get started! ðŸŽ‰\n",
      "\n",
      "Excellent choice! A car trip generally has a higher carbon footprint than a train trip because cars produce more emissions per passenger than trains do. Cars also tend to travel shorter distances than trains, which means they have fewer opportunities to reach high speeds and optimize their fuel efficiency.\n",
      "\n",
      "Did you know that a train can carry hundreds of passengers and still produce significantly less emissions than a single car? ðŸš‚ðŸ’¨ This is because trains are designed to be more efficient and use cleaner energy sources, such as electricity or diesel fuel.\n",
      "\n",
      "So, while both modes of transportation have their own carbon footprint, trains are generally the better choice for reducing your impact on the environment. ðŸŒŽ Do you want to keep playing the quiz or stop here?\n"
     ]
    }
   ],
   "source": [
    "messages.append(\n",
    "    (\"human\", human_answer)\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | llm | output_parser\n",
    "response = chain.invoke(items)\n",
    "messages.append(\n",
    "    (\"ai\", response)\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2006b781-d178-4b94-9286-d5e3e512ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human answer (this time we let the actual human answer)\n",
    "human_answer = \"No, I'm fine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0954e148-d4df-481e-a343-9f2a7edd301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP\n"
     ]
    }
   ],
   "source": [
    "messages_continue_or_stop = [\n",
    "    (\"system\", \"\"\"Your task is to find if the human wants to keep playing a quiz or not.\n",
    "    If he wants to keep playing, you write exactly 'CONTINUE'.\n",
    "    If he doesn't want to keep playing, you write exactly 'STOP'.\n",
    "    \"\"\"),\n",
    "    (\"ai\", response),\n",
    "    (\"human\", human_answer),\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages_continue_or_stop)\n",
    "chain = prompt | llm | output_parser\n",
    "response_continue_or_stop = chain.invoke({})\n",
    "print(response_continue_or_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f8edde-b730-4dde-aec5-acca80ed206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Since you want to stop here, let's summarize what we've learned so far:\n",
      "\n",
      "* A train trip generally has a lower carbon footprint than a car trip.\n",
      "* Trains are more efficient and use cleaner energy sources than cars.\n",
      "* While both modes of transportation have their own carbon footprint, trains are generally the better choice for reducing your impact on the environment.\n",
      "\n",
      "Thanks for playing the quiz with me! If you want to play again or try a different type of quiz, just let me know. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "messages.append(\n",
    "    (\"human\", human_answer)\n",
    ")\n",
    "if response_continue_or_stop == \"STOP\":\n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    chain = prompt | llm | output_parser\n",
    "    response = chain.invoke(items)\n",
    "    messages.append(\n",
    "        (\"ai\", response)\n",
    "    )\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"On change les items et c'est reparti pour un tour.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d34aaf-805f-437a-a8ba-57dfab080f42",
   "metadata": {},
   "source": [
    "# To make it easier to have a quiz with multiple questions, let's make all this into a nice function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874507b-b0dd-46c5-9a29-6c379ec6195a",
   "metadata": {},
   "source": [
    "NB : I tweaked the prompt a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4eb7392-c7e9-486a-9ad5-e4a147d33c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "083773d3-8896-4123-b853-763a7854e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quiz_bot(llm, items: dict, say_hello: bool):\n",
    "    max_question_count = 5\n",
    "    output_parser = StrOutputParser()\n",
    "    if say_hello:\n",
    "        human_answer = \"Hello, who are you?\"\n",
    "    else:\n",
    "        human_answer = \"What's your question?\"\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are quiz presentator and you always speak in less than 50 words.\n",
    "        1- You ask one short question about '{item1}' versus '{item2}': you ask which has the highest carbon footprint.\n",
    "        2- After the human replied, you give a short answer with short explainations.\n",
    "        3- Then you ask the human if he wants to keep playing the quizz or not.\n",
    "        \"\"\"),\n",
    "        (\"human\", human_answer),\n",
    "    ]\n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    chain = prompt | llm | output_parser\n",
    "    response = chain.invoke(items)\n",
    "    messages.append(\n",
    "        (\"ai\", response)\n",
    "    )\n",
    "    print(response)\n",
    "    print(\"---\")\n",
    "    human_answer = input()\n",
    "    print(\"---\")\n",
    "    messages.append(\n",
    "        (\"human\", human_answer)\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    chain = prompt | llm | output_parser\n",
    "    response = chain.invoke(items)\n",
    "    messages.append(\n",
    "        (\"ai\", response)\n",
    "    )\n",
    "    print(response)\n",
    "    print(\"---\")\n",
    "    human_answer = input()\n",
    "    print(\"---\")\n",
    "    messages_continue_or_stop = [\n",
    "        (\"system\", \"\"\"Your task is to find if the human wants to keep playing a quiz or not.\n",
    "        If he wants to keep playing, you write exactly 'CONTINUE'.\n",
    "        If he doesn't want to keep playing, you write exactly 'STOP'.\n",
    "        \"\"\"),\n",
    "        (\"ai\", response),\n",
    "        (\"human\", human_answer),\n",
    "    ]\n",
    "    prompt = ChatPromptTemplate.from_messages(messages_continue_or_stop)\n",
    "    chain = prompt | llm | output_parser\n",
    "    response_continue_or_stop = chain.invoke({})\n",
    "    messages.append(\n",
    "    (\"human\", human_answer)\n",
    "    )\n",
    "    if response_continue_or_stop == \"STOP\":\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        chain = prompt | llm | output_parser\n",
    "        response = chain.invoke(items)\n",
    "        messages.append(\n",
    "            (\"ai\", response)\n",
    "        )\n",
    "        print(response)\n",
    "    return response_continue_or_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c83b5a-dde7-4ce6-a8b4-018c9158dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_list = [\n",
    "    {\n",
    "    \"item1\": \"train trip\",\n",
    "    \"item2\": \"car trip\",\n",
    "    },\n",
    "    {\n",
    "    \"item1\": \"steak\",\n",
    "    \"item2\": \"veggie bowl\",\n",
    "    },\n",
    "    {\n",
    "    \"item1\": \"France\",\n",
    "    \"item2\": \"New Zealand\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97354942-e55b-4bf4-a991-f0d5d9731a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello there! *winks* I'm your quiz presenter today, and I'll be keeping things short and sweet. ðŸ˜Š\n",
      "\n",
      "Question 1: Train trip or car trip - which has the highest carbon footprint? ðŸš‚ðŸš— Please choose wisely!\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I think it's the car\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Great job, human! You are correct. Car trips generally have a higher carbon footprint than train trips due to the amount of fuel consumed by cars. ðŸŒŽ\n",
      "\n",
      "Would you like to play another question or stop here? ðŸ¤”\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " yes, play another one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\"Which option has the highest carbon footprint: steak or veggie bowl?\"\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " steak for sure!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "\n",
      "Great, let's dive into this question! A steak typically has a higher carbon footprint than a veggie bowl due to the resources required to produce and transport it. The production of beef, for example, can lead to greenhouse gas emissions and deforestation. On the other hand, a veggie bowl made with locally sourced ingredients has a lower carbon footprint.\n",
      "\n",
      "Would you like to keep playing the quiz or stop here?\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " no, stop here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Great, thank you for playing the quiz! It's important to be aware of the environmental impact of our food choices and make conscious decisions to reduce our carbon footprint. If you have any other questions or would like to continue playing, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "for items, say_hello in zip(items_list, [True, False, False]):\n",
    "    response_continue_or_stop = quiz_bot(llm, items, say_hello)\n",
    "    if response_continue_or_stop == \"STOP\":\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
